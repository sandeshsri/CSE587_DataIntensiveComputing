{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_y in main\n",
      "(32764,)\n"
     ]
    }
   ],
   "source": [
    "#Reading datafile\n",
    "data = pd.read_csv(\"data.csv\", header = None)\n",
    "data = data.drop(index=0) #Drops the column names\n",
    "#print(data.shape) #dimensions of the data\n",
    "#print(data)\n",
    "\n",
    "#Features and label extraction\n",
    "data_x = data.iloc[:, :48] #Features\n",
    "data_y = data.iloc[:, 48] #Labels\n",
    "from sklearn import preprocessing\n",
    "MinMaxScaler = preprocessing.MinMaxScaler()\n",
    "data_x = MinMaxScaler.fit_transform(data_x)\n",
    "\n",
    "#Splittiing data into train (80%) and test (20%)\n",
    "train_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size = 0.2)\n",
    "print(\"train_y in main\")\n",
    "print(train_y.shape)\n",
    "#print(data.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Accuracy(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    :type y_true: numpy.ndarray\n",
    "    :type y_pred: numpy.ndarray\n",
    "    :rtype: float    \n",
    "    \"\"\"\n",
    "    cm = ConfusionMatrix(y_true,y_pred)\n",
    "    correct = np.trace(cm)\n",
    "    total = cm.sum()\n",
    "    acc=0\n",
    "    if(total>0):\n",
    "        acc = correct/total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConfusionMatrix(y_true,y_pred):\n",
    "    \"\"\"\n",
    "    :type y_true: numpy.ndarray\n",
    "    :type y_pred: numpy.ndarray\n",
    "    :rtype: float\n",
    "    \"\"\"\n",
    "    print(\"y_true\")\n",
    "    print(y_true.shape)\n",
    "    print(\"y_pred\")\n",
    "    print(y_pred.shape)\n",
    "    #For 0-10 range\n",
    "    un = np.unique(y_true)\n",
    "    y_true=y_true-un[0]\n",
    "    un = np.unique(y_pred)\n",
    "    y_pred=y_pred-un[0]\n",
    "    classes = len(np.unique(np.concatenate((y_true,y_pred))))        \n",
    "    mul = y_true*classes\n",
    "    sum1 = mul+y_pred    \n",
    "    x,bins= np.histogram(sum1,bins=np.arange(classes**2+1))\n",
    "    mat = x.reshape(classes, classes)\n",
    "    return(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(X_train,N):\n",
    "    \"\"\"\n",
    "    :type X_train: numpy.ndarray\n",
    "    :type N: int\n",
    "    :rtype: numpy.ndarray\n",
    "    \"\"\"\n",
    "    #standardizing data \n",
    "    sc = StandardScaler()\n",
    "    std_data = sc.fit_transform(X_train)\n",
    "    pca_data = pd.DataFrame(std_data)\n",
    "\n",
    "    #Construct Covariance Matrix(C)\n",
    "    transpose_data = pca_data.T\n",
    "    C = np.cov(transpose_data)\n",
    "\n",
    "\n",
    "    #Decomposing C to eigen vectors and values\n",
    "    e_val, e_vec =  np.linalg.eig(C)\n",
    "\n",
    "\n",
    "    #N = int(input(\"Reduced Deminsions:\")) ------ Passed on to tge function\n",
    "\n",
    "    #Estimating High-valued Eigen vectors\n",
    "    eval_sorted = np.argsort(e_val)\n",
    "    top_evals = eval_sorted[::-1][:N]\n",
    "    high_valued_vectors = e_vec[:,top_evals]\n",
    "\n",
    "    PCA = np.empty([pca_data.shape[0], high_valued_vectors.shape[1]])\n",
    "    i = 0\n",
    "    for v in high_valued_vectors.T:\n",
    "            PCA[:,i] = np.dot(pca_data, v.T)\n",
    "            i += 1\n",
    "    return PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2]\n",
      " [2]\n",
      " [2]\n",
      " ...\n",
      " [2]\n",
      " [2]\n",
      " [2]]\n",
      "y_true\n",
      "(32764, 1)\n",
      "y_pred\n",
      "(32764, 1)\n",
      "Random Forest accuracy : 0.257020\n",
      "y_true\n",
      "(32764, 1)\n",
      "y_pred\n",
      "(32764, 1)\n",
      "random Random Forest accuracy : 0.090313\n"
     ]
    }
   ],
   "source": [
    "def gini(samples, list_ind):\n",
    "    n_samples = len(samples)\n",
    "    if n_samples != 0:\n",
    "        class_ind = [dp[-1] for dp in samples]\n",
    "        gini_index = 0.0\n",
    "        for index in list_ind:\n",
    "            prob = class_ind.count(index) / n_samples\n",
    "            gini_index += prob ** 2\n",
    "        return 1.0 - gini_index\n",
    "    return 0\n",
    "\n",
    "def entropy(samples, index_list):\n",
    "    n_samples = len(samples)\n",
    "    if n_samples != 0:\n",
    "        labels = [dp[-1] for dp in samples]\n",
    "        sum_ = 0.0\n",
    "        for index in index_list:\n",
    "            prob = labels.count(index) / n_samples\n",
    "            sum_ -= prob * np.log2(prob)\n",
    "        return sum_\n",
    "    return 0\n",
    "\n",
    "def gini_gain(samples, list_ind):\n",
    "    n_samples = sum([len(dp) for dp in samples])\n",
    "    data = [sample for dp in samples for sample in dp]\n",
    "    gini_impurity = gini(data, list_ind) #Gini index of parent node\n",
    "    for dp in samples:\n",
    "        gini_impurity -= gini(dp, list_ind) * len(dp) / n_samples #Gini index of child node\n",
    "    return gini_impurity\n",
    "\n",
    "def feature_split(node, max_depth, min_size, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    if not left or not right:\n",
    "        cls_list = [row[-1] for row in (left + right)]\n",
    "        node['left'] = node['right'] = max(set(cls_list), key = cls_list.count)\n",
    "        return\n",
    "    if depth >= max_depth:\n",
    "        left_list = [row[-1] for row in left]\n",
    "        node['left'] = max(set(left_list), key = left_list.count)\n",
    "        right_list = [row[-1] for row in right]\n",
    "        node['right'] = max(set(right_list), key = right_list.count)\n",
    "        return\n",
    "    if len(left) <= min_size:\n",
    "        left_list = [row[-1] for row in left]\n",
    "        node['left'] = max(set(left_list), key = left_list.count)\n",
    "    else:\n",
    "        feature_index = int( np.random.random() * (len(right[0]) - 1) )\n",
    "        node['left'] = optimal_split(left, feature_index)\n",
    "        feature_split(node['left'], max_depth, min_size, depth + 1)\n",
    "\n",
    "    if len(right) <= min_size:\n",
    "        right_list = [row[-1] for row in right]\n",
    "        node['right'] = max(set(right_list), key = right_list.count)\n",
    "    else:\n",
    "        feature_index = int( np.random.random() * (len(right[0]) - 1) )\n",
    "        node['right'] = optimal_split(right, feature_index)\n",
    "        feature_split(node['right'], max_depth, min_size, depth + 1)\n",
    "\n",
    "def optimal_split(samples, index):\n",
    "    max_gain = -1.0\n",
    "    list_ind = list(set(dp[-1] for dp in samples))\n",
    "    value = 0.0\n",
    "    split_groups = None\n",
    "    for dp in samples:\n",
    "        left = []\n",
    "        right = []\n",
    "        for sample in samples:\n",
    "            if sample[index] < dp[index]:\n",
    "                left.append(sample)\n",
    "            else:\n",
    "                right.append(sample)\n",
    "        sample_set = [left, right]\n",
    "        info_gain = gini_gain(sample_set, list_ind)\n",
    "        if info_gain > max_gain:\n",
    "            value, max_gain, split_groups = dp[index], info_gain, sample_set\n",
    "    return { 'index': index, 'split_value': value, 'groups': sample_set }\n",
    "\n",
    "def DecisionTree(samples, max_depth, min_size):\n",
    "    max_gain = -1.0\n",
    "    list_ind = list(set(row[-1] for row in samples))\n",
    "    value = 0.0\n",
    "    split_groups = None\n",
    "    feature_ind = int(np.random.random() * (len(samples[0]) - 1) )\n",
    "    for row in samples:\n",
    "        left = []\n",
    "        right = []\n",
    "        for sample in samples:\n",
    "            if sample[feature_ind] < row[feature_ind]:\n",
    "                left.append(sample)\n",
    "            else:\n",
    "                right.append(sample)\n",
    "        data = [left, right]\n",
    "        info_gain = gini_gain(data, list_ind)\n",
    "        if info_gain > max_gain:\n",
    "            value, max_gain, split_groups = row[feature_ind], info_gain, data \n",
    "    tree = {'index': feature_ind,'split_value': value,'groups': data }\n",
    "    feature_split(tree, max_depth, min_size, 1)\n",
    "    return tree\n",
    "\n",
    "def tree_nodes(feature, samples):\n",
    "    if samples[feature['index']] < feature['split_value']:\n",
    "        if isinstance(feature['left'], dict):\n",
    "            return tree_nodes(feature['left'], samples)\n",
    "        else:\n",
    "            return feature['left']\n",
    "    elif isinstance(feature['right'], dict):\n",
    "        return tree_nodes(feature['right'], samples)\n",
    "    else:\n",
    "        return feature['right']\n",
    "    \n",
    "def RandomForest(X_train,Y_train,X_test,Y_test):\n",
    "    \"\"\"\n",
    "    :type X_train: numpy.ndarray\n",
    "    :type X_test: numpy.ndarray\n",
    "    :type Y_train: numpy.ndarray\n",
    "    \n",
    "    :rtype: numpy.ndarray\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    trees = []\n",
    "    n_trees = 200\n",
    "    n_features = X_train.shape[1]\n",
    "    max_depth = 10\n",
    "    min_samples = 5\n",
    "    data = np.concatenate((X_train, Y_train), axis = 1)\n",
    "\n",
    "    for i in range(n_trees):\n",
    "        #Random selection of features for decision trees\n",
    "        feature_ind = np.random.choice(len(data), n_features)\n",
    "        trees.append(DecisionTree(data[feature_ind], max_depth, min_samples))\n",
    "        \n",
    "    for sample in X_train:\n",
    "        classes = []\n",
    "        for tree in trees:\n",
    "            classes.append(tree_nodes(tree, sample))\n",
    "        labels.append(int(max(set(classes), key = classes.count)))    \n",
    "    return labels\n",
    "\n",
    "pred = RandomForest(train_x, train_y, test_x, test_y)\n",
    "pred = np.array(pred)\n",
    "pred = pred.reshape(pred.shape[0], 1)\n",
    "rand_pred = np.random.randint(1,11, size=(32764, 1))\n",
    "rand_pred = np.ones((32764,1),dtype=int)\n",
    "rand_pred = np.full((32764, 1), 2)\n",
    "print(rand_pred)\n",
    "print(\"Random Forest accuracy : %f\" %Accuracy(train_y, pred))\n",
    "print(\"random Random Forest accuracy : %f\" %Accuracy(train_y, rand_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trin_y\n",
      "(32764,)\n",
      "train_y\n",
      "(32764, 1)\n",
      "train_X\n",
      "(32764, 48)\n",
      "Y_train\n",
      "(32764, 1)\n",
      "Unique for Y-train\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11]\n",
      "[2959 2998 3020 2968 2977 2986 2952 2969 2955 2997 2983]\n",
      "2\n",
      "Unique\n",
      "[ 1  2  3  6  7  8  9 10 11]\n",
      "[ 3206  2413   202     1   115 23007   758    13  3049]\n",
      "y_true\n",
      "(32764, 1)\n",
      "y_pred\n",
      "(32764, 1)\n",
      "Random Forest accuracy : 0.232053\n"
     ]
    }
   ],
   "source": [
    "def gini(group, list_of_class_ids):\n",
    "    Ngroup = len(group)\n",
    "    #print(\"Ngroup\")\n",
    "    #print(Ngroup)\n",
    "    if Ngroup == 0:\n",
    "        return 0\n",
    "    dataset_class_ids = [row[-1] for row in group]\n",
    "    sum_over_classes = 0.\n",
    "    for class_id in list_of_class_ids:\n",
    "        prob = dataset_class_ids.count(class_id)/Ngroup\n",
    "        sum_over_classes += prob**2\n",
    "    return 1. - sum_over_classes\n",
    "\n",
    "def split(node, max_depth, min_size, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'] = to_terminal(left)\n",
    "        node['right'] = to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        feature_index = int( np.random.random()*(len(right[0]) - 1) )\n",
    "        node['left'] = get_split(left, feature_index)\n",
    "        split(node['left'], max_depth, min_size, depth+1)\n",
    "    # process right child\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        feature_index = int( np.random.random()*(len(right[0]) - 1) )\n",
    "        node['right'] = get_split(right, feature_index)\n",
    "        split(node['right'], max_depth, min_size, depth+1)\n",
    "        \n",
    "def calc_information_gain(groups, list_of_class_ids):\n",
    "    # count all samples\n",
    "    Nall = sum([len(group) for group in groups])\n",
    "    \n",
    "    # calculate Gini index of parent node\n",
    "    all_rows = [row for group in groups for row in group]\n",
    "    IG = gini(all_rows, list_of_class_ids)\n",
    "    \n",
    "    # calculate Gini index of daughter nodes\n",
    "    for group in groups:\n",
    "        IG -= gini(group, list_of_class_ids)*len(group)/Nall\n",
    "    return IG\n",
    "\n",
    "def split_node(index, value, dataset):\n",
    "    ''' Split the dataset into two using a feature index and \n",
    "    feature value '''\n",
    "    left = []\n",
    "    right = []\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return [left, right]\n",
    "\n",
    "def get_split(dataset, index):\n",
    "    ''' Evaluate all possible splits given the dataset and the index of \n",
    "    the feature on which to split '''\n",
    "    list_of_class_ids = list(set(row[-1] for row in dataset))\n",
    "    split_value, max_IG, split_groups = 0., -1., None\n",
    "    for row in dataset:\n",
    "        groups = split_node(index, row[index], dataset)\n",
    "        IG = calc_information_gain(groups, list_of_class_ids)\n",
    "        if IG > max_IG:\n",
    "            split_value, max_IG, split_groups = row[index], IG, groups\n",
    "    return {'index': index,'split_value': split_value,'groups': groups }\n",
    "\n",
    "def build_tree(train, max_depth, min_size):\n",
    "    # randomly determine the feature index\n",
    "    feature_index = int( np.random.random()*(len(train[0]) - 1) )\n",
    "    #print(\"initial feature index\")\n",
    "    #print(feature_index)\n",
    "    #print(\"PCA\")\n",
    "    trainb = np.delete(train, -1, axis=1)\n",
    "    ret = PCA(trainb,1)\n",
    "    #print(ret.shape)\n",
    "    #print(ret)\n",
    "    ret = ret.flatten()\n",
    "    #print(ret.shape)\n",
    "    feature_index = np.argmax(ret)\n",
    "    #print(\"new feature index\")\n",
    "    #print(feature_index)\n",
    "    \n",
    "    root = get_split(train, feature_index)\n",
    "    split(root, max_depth, min_size, 1)\n",
    "    return root\n",
    "\n",
    "def to_terminal(group):\n",
    "    # Create a terminal node value\n",
    "    list_of_classes = [row[-1] for row in group]\n",
    "    return max(set(list_of_classes), key=list_of_classes.count)\n",
    "\n",
    "def traverse_tree(node, row):\n",
    "    if row[node['index']] < node['split_value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return traverse_tree(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return traverse_tree(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "\n",
    "def RandomForest(X_train,Y_train,X_test,Y_test):\n",
    "    \"\"\"\n",
    "    :type X_train: numpy.ndarray\n",
    "    :type X_test: numpy.ndarray\n",
    "    :type Y_train: numpy.ndarray\n",
    "    \n",
    "    :rtype: numpy.ndarray\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    n_trees = 200\n",
    "    n_features = X_train.shape[1]\n",
    "    max_depth = 35\n",
    "    min_samples = 5\n",
    "    \n",
    "    print(\"train_X\")\n",
    "    print(X_train.shape)\n",
    "    \n",
    "    print(\"Y_train\")\n",
    "    print(Y_train.shape)\n",
    "    \n",
    "    data = np.concatenate((X_train, Y_train), axis = 1)\n",
    "    c, cot = np.unique(Y_train, return_counts = True)\n",
    "    print(\"Unique for Y-train\")\n",
    "    print(c)\n",
    "    print(cot)\n",
    "    print(np.argmax(cot))\n",
    "    \n",
    "    forest = []\n",
    "    for i in range(n_trees):\n",
    "        feature_ind = np.random.choice(len(data), n_features)\n",
    "        forest.append(build_tree(data[feature_ind], max_depth, min_samples))\n",
    "        \n",
    "    for sample in X_train:\n",
    "        classes = []\n",
    "        for root in forest:\n",
    "            classes.append(traverse_tree(root, sample))\n",
    "        labels.append(int(max(set(classes), key = classes.count)))\n",
    "        \n",
    "    return labels\n",
    "\n",
    "print(\"trin_y\")\n",
    "print(train_y.shape)\n",
    "train_y = train_y.values\n",
    "train_y = train_y.reshape(train_y.shape[0],1)\n",
    "print(\"train_y\")\n",
    "print(train_y.shape)\n",
    "\n",
    "pred = RandomForest(train_x, train_y, test_x, test_y)\n",
    "pred = np.array(pred)\n",
    "pred = pred.reshape(pred.shape[0],1)\n",
    "c, cot = np.unique(pred, return_counts = True)\n",
    "print(\"Unique\")\n",
    "print(c)\n",
    "print(cot)\n",
    "print(\"Random Forest accuracy : %f\" %Accuracy(train_y, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
